common:
  action_space_lb: &ACTION_SPACE_LB [-4.5, -0.5235987755983]
  action_space_ub: &ACTION_SPACE_UB [+4.5, +0.5235987755983]

environment:
  type: 'TrajectoryTrackingEnv'
  step_interval: 0.1
  # TODO: add step index/ total number to observation
  #   to deal with variance of cumulative reward w.r.t. different trajectory length
  tracking_length_lb: 50
  tracking_length_ub: 50
  init_state_lb: [-1.e-8, -1.e-8, -3.1415926536, 5.0]
  init_state_ub: [+1.e-8, +1.e-8, +3.1415926536, 30.0]
  n_observation_steps: 15
  dynamics_model_configs:
      # short vehicle
  -   type: 'BicycleModel'
      front_overhang: 0.9
      rear_overhang: 0.9
      wheelbase: 2.7
      width: 1.8
      action_space_lb: *ACTION_SPACE_LB
      action_space_ub: *ACTION_SPACE_UB
      # long vehicle
  -   type: 'BicycleModel'
      front_overhang: 2.3
      rear_overhang: 2.0
      wheelbase: 6.1
      width: 2.5
      action_space_lb: *ACTION_SPACE_LB
      action_space_ub: *ACTION_SPACE_UB
      # middle vehicle
  -   type: 'BicycleModel'
      front_overhang: 1.0
      rear_overhang: 1.5
      wheelbase: 3.4
      width: 2.65
      action_space_lb: *ACTION_SPACE_LB
      action_space_ub: *ACTION_SPACE_UB

model:
  type: TD3
  policy: MlpPolicy
  policy_kwargs:
    net_arch:
      pi: [128, 32]
      qf: [1024, 512, 256, 128]
  train_freq: [4, "episode"]
  learning_rate: 4.0e-5
  scaled_action_noise:
    mean: [0.0, 0.0]
    sigma: [0.9, 0.3]
  verbose: 1
  learning_starts: 4096
  batch_size: 4096
  tau: 0.001

learning:
    total_timesteps: 3_000_000
    log_interval: 10

evaluation:
  n_episodes: 200
  compute_metrics_name: "compute_bicycle_model_metrics"
