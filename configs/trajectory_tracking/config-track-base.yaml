common:
  action_space_lb: &ACTION_SPACE_LB [-4.5, -0.5235987755983]
  action_space_ub: &ACTION_SPACE_UB [+4.5, +0.5235987755983]

environment:
  type: 'TrajectoryTrackingEnv'
  step_interval: 0.1
  # TODO: add step index/ total number to observation
  #   to deal with variance of cumulative reward w.r.t. different trajectory length
  tracking_length_lb: 50
  tracking_length_ub: 50
  reference_line_pad_mode: 'repeat'
  init_state_lb: [-1.e-8, -1.e-8, -3.1415926536, 0.1]
  init_state_ub: [+1.e-8, +1.e-8, +3.1415926536, 40.0]
  n_observation_steps: 15
  dynamics_model_configs:
  -   type: 'BicycleModel'
      name: 'ShortVehicle'
      front_overhang: 0.9
      rear_overhang: 0.9
      wheelbase: 2.7
      width: 1.8
      action_space_lb: *ACTION_SPACE_LB
      action_space_ub: *ACTION_SPACE_UB
      max_lat_acc: 8.0
  -   type: 'BicycleModel'
      name: 'LongVehicle'
      front_overhang: 2.3
      rear_overhang: 2.0
      wheelbase: 6.1
      width: 2.5
      action_space_lb: *ACTION_SPACE_LB
      action_space_ub: *ACTION_SPACE_UB
      max_lat_acc: 4.0
  -   type: 'BicycleModel'
      name: 'MiddleVehicle'
      front_overhang: 1.0
      rear_overhang: 1.5
      wheelbase: 3.4
      width: 2.65
      action_space_lb: *ACTION_SPACE_LB
      action_space_ub: *ACTION_SPACE_UB
      max_lat_acc: 2.0

algorithm:
  type: TD3
  policy: MlpPolicy
  train_freq: [4, "episode"]
  learning_rate: 1.0e-4
  scaled_action_noise:
    mean: [0.0, 0.0]
    sigma: [0.9, 0.3]
  verbose: 1
  learning_starts: 4096
  batch_size: 4096
  tau: 0.0001

learning:
    total_timesteps: 1_000_000
    log_interval: 10

evaluation:
  eval_config:
    n_episodes: 200
    compute_metrics_name: "compute_bicycle_model_metrics"
  overriden_environment: {}
